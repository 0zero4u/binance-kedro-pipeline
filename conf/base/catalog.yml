# 01_raw
book_raw:
  type: kedro_datasets.pandas.CSVDataset
  filepath: "data/01_raw/book_ticker.csv"
  load_args:
    names: ["update_id", "best_bid_price", "best_bid_qty", "best_ask_price", "best_ask_qty", "transaction_time", "event_time"]
    skiprows: 1 
    dtype:
      update_id: int64
      best_bid_price: float64
      best_bid_qty: float64
      best_ask_price: float64
      best_ask_qty: float64
      transaction_time: int64
      event_time: int64

trade_raw:
  type: kedro_datasets.pandas.CSVDataset
  filepath: "data/01_raw/trades.csv"
  load_args:
    names: ["id", "price", "qty", "quote_qty", "time", "is_buyer_maker"]
    skiprows: 1
    dtype:
      id: int64
      price: float64
      qty: float64
      quote_qty: float64
      time: int64
      is_buyer_maker: bool

# 02_intermediate
merged_tick_data: {type: kedro_datasets.pandas.ParquetDataset, filepath: "data/02_intermediate/merged_tick_data.parquet"}
enriched_tick_data_unvalidated: {type: MemoryDataSet}
enriched_tick_data: {type: MemoryDataSet}
ewma_features_tbt: {type: MemoryDataSet}

# 03_primary
features_data_unvalidated: {type: MemoryDataSet}
features_data: {type: kedro_datasets.pandas.ParquetDataset, filepath: "data/03_primary/features_data.parquet"}

# 05_model_input (LGBM)
labeled_data:
  type: kedro_datasets.pandas.ParquetDataset
  filepath: "data/05_model_input/labeled_data.parquet"
  layer: model_input

# 06_models (LGBM)
lgbm_model:
  type: kedro_datasets.pickle.PickleDataset
  filepath: "data/06_models/lgbm_price_movement_model.pkl"
  layer: models

# 07_reporting (LGBM & New Traceability Report)
lgbm_eval_results:
  type: kedro_datasets.json.JSONDataset
  filepath: "data/07_reporting/lgbm_evaluation.json"

unified_5min_traceability_report:
  type: kedro_datasets.pandas.CSVDataset
  filepath: "data/07_reporting/unified_5min_traceability_report.csv"
  layer: reporting

# ----------------------------------------------------------------
# Datasets for the ARF Pipeline
# ----------------------------------------------------------------
features_data_arf: {type: kedro_datasets.pandas.ParquetDataset, filepath: data/03_primary/features_data_arf.parquet}
labeled_data_arf: {type: kedro_datasets.pandas.ParquetDataset, filepath: data/05_model_input/labeled_data_arf.parquet}
scaled_data_arf: {type: kedro_datasets.pandas.ParquetDataset, filepath: data/05_model_input/scaled_data_arf.parquet}
robust_scaler: {type: kedro_datasets.pickle.PickleDataset, filepath: data/06_models/robust_scaler.pkl}
arf_training_results: {type: kedro_datasets.pickle.PickleDataset, filepath: data/06_models/arf_training_results.pkl}
arf_model: {type: kedro_datasets.pickle.PickleDataset, filepath: data/06_models/arf_price_movement_model.pkl, layer: models}
--- START OF FILE binance-kedro-pipeline-main/conf/base/logging.yml ---

version: 1
disable_existing_loggers: False

formatters:
  simple:
    format: "%(asctime)s - %(name)s - [%(levelname)s] %(message)s"
  rich:
    format: "%(message)s"
    datefmt: "[%X]"

handlers:
  # New handler for beautiful, live console logging
  rich_console:
    class: kedro.logging.RichHandler
    level: INFO
    formatter: rich
    rich_tracebacks: true
    show_path: false
    markup: true

  # New handler to save detailed logs to a file
  info_file_handler:
    class: logging.handlers.RotatingFileHandler
    level: DEBUG # Log everything to the file
    formatter: simple
    filename: logs/info.log
    maxBytes: 10485760 # 10MB
    backupCount: 3
    encoding: utf8
    mode: a

# The root logger captures all logs
root:
  level: DEBUG # Set root to DEBUG to capture everything for the file handler
  handlers: [rich_console, info_file_handler]
